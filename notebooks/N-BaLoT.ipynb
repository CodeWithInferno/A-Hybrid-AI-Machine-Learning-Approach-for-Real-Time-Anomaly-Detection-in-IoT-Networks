{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e876285-ac6c-47c9-a972-654a5649178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 92 files. Sampling 5.0% from each...\n",
      "Processing file 1/92: 7.gafgyt.combo.csv\n",
      "Processing file 2/92: 6.mirai.scan.csv\n",
      "Processing file 3/92: 5.gafgyt.junk.csv\n",
      "Processing file 4/92: 5.gafgyt.combo.csv\n",
      "Processing file 5/92: 9.mirai.syn.csv\n",
      "Processing file 7/92: 9.mirai.scan.csv\n",
      "Processing file 8/92: 9.mirai.udp.csv\n",
      "Processing file 9/92: 8.mirai.udp.csv\n",
      "Processing file 10/92: 1.gafgyt.combo.csv\n",
      "Processing file 11/92: 1.benign.csv\n",
      "Processing file 12/92: 1.mirai.syn.csv\n",
      "Processing file 13/92: 3.gafgyt.combo.csv\n",
      "Processing file 14/92: 9.mirai.udpplain.csv\n",
      "Processing file 15/92: 6.mirai.syn.csv\n",
      "Processing file 16/92: 4.mirai.syn.csv\n",
      "Processing file 17/92: 9.benign.csv\n",
      "Processing file 18/92: 8.gafgyt.tcp.csv\n",
      "Processing file 19/92: 9.mirai.ack.csv\n",
      "Processing file 20/92: 5.gafgyt.scan.csv\n",
      "Processing file 21/92: 2.mirai.syn.csv\n",
      "Processing file 22/92: 6.gafgyt.tcp.csv\n",
      "Processing file 23/92: 7.benign.csv\n",
      "Processing file 24/92: 8.gafgyt.udp.csv\n",
      "Processing file 25/92: 5.mirai.ack.csv\n",
      "Processing file 26/92: 7.gafgyt.junk.csv\n",
      "Processing file 27/92: 2.mirai.scan.csv\n",
      "Processing file 28/92: 8.mirai.syn.csv\n",
      "Processing file 29/92: 6.gafgyt.udp.csv\n",
      "Processing file 30/92: 4.gafgyt.combo.csv\n",
      "Processing file 31/92: 5.gafgyt.udp.csv\n",
      "Processing file 32/92: 6.benign.csv\n",
      "Processing file 33/92: 6.mirai.udpplain.csv\n",
      "Processing file 34/92: 8.mirai.udpplain.csv\n",
      "Processing file 35/92: 4.mirai.udpplain.csv\n",
      "Processing file 36/92: 4.benign.csv\n",
      "Processing file 37/92: 1.gafgyt.tcp.csv\n",
      "Processing file 38/92: 2.gafgyt.udp.csv\n",
      "Processing file 39/92: 3.gafgyt.junk.csv\n",
      "Processing file 40/92: 3.gafgyt.tcp.csv\n",
      "Processing file 41/92: 6.gafgyt.combo.csv\n",
      "Processing file 42/92: 9.gafgyt.tcp.csv\n",
      "Processing file 43/92: 5.gafgyt.tcp.csv\n",
      "Processing file 44/92: 4.gafgyt.tcp.csv\n",
      "Processing file 45/92: 7.gafgyt.tcp.csv\n",
      "Processing file 46/92: 3.benign.csv\n",
      "Processing file 47/92: 5.mirai.udpplain.csv\n",
      "Processing file 48/92: 6.mirai.ack.csv\n",
      "Processing file 49/92: 1.mirai.ack.csv\n",
      "Processing file 50/92: 1.mirai.udpplain.csv\n",
      "Processing file 51/92: 2.gafgyt.scan.csv\n",
      "Processing file 52/92: 2.gafgyt.junk.csv\n",
      "Processing file 53/92: 9.gafgyt.junk.csv\n",
      "Processing file 54/92: 4.gafgyt.udp.csv\n",
      "Processing file 55/92: 2.gafgyt.tcp.csv\n",
      "Processing file 56/92: 2.mirai.udp.csv\n",
      "Processing file 57/92: 6.gafgyt.junk.csv\n",
      "Processing file 58/92: 9.gafgyt.scan.csv\n",
      "Processing file 59/92: 9.gafgyt.udp.csv\n",
      "Processing file 60/92: 1.gafgyt.junk.csv\n",
      "Processing file 61/92: 8.gafgyt.combo.csv\n",
      "Processing file 62/92: 3.gafgyt.scan.csv\n",
      "Processing file 63/92: 8.gafgyt.scan.csv\n",
      "Processing file 64/92: 4.gafgyt.scan.csv\n",
      "Processing file 65/92: 9.gafgyt.combo.csv\n",
      "Processing file 66/92: 5.mirai.syn.csv\n",
      "Processing file 67/92: 7.gafgyt.scan.csv\n",
      "Processing file 68/92: 5.benign.csv\n",
      "Processing file 69/92: 4.mirai.udp.csv\n",
      "Processing file 70/92: 3.gafgyt.udp.csv\n",
      "Processing file 71/92: 2.benign.csv\n",
      "Processing file 73/92: 2.gafgyt.combo.csv\n",
      "Processing file 74/92: 7.gafgyt.udp.csv\n",
      "Processing file 75/92: 1.gafgyt.udp.csv\n",
      "Processing file 76/92: 8.mirai.scan.csv\n",
      "Processing file 77/92: 4.gafgyt.junk.csv\n",
      "Processing file 78/92: 5.mirai.scan.csv\n",
      "Processing file 79/92: 8.gafgyt.junk.csv\n",
      "Processing file 80/92: 6.gafgyt.scan.csv\n",
      "Processing file 81/92: 2.mirai.udpplain.csv\n",
      "Processing file 82/92: 5.mirai.udp.csv\n",
      "Processing file 83/92: 1.mirai.udp.csv\n",
      "Processing file 84/92: 8.mirai.ack.csv\n",
      "Processing file 85/92: 1.mirai.scan.csv\n",
      "Processing file 86/92: 2.mirai.ack.csv\n",
      "Processing file 87/92: 1.gafgyt.scan.csv\n",
      "Processing file 88/92: 4.mirai.ack.csv\n",
      "Processing file 89/92: 8.benign.csv\n",
      "Processing file 90/92: 4.mirai.scan.csv\n",
      "Processing file 92/92: 6.mirai.udp.csv\n",
      "\n",
      "Combining all samples...\n",
      "\n",
      "Combined sample created successfully!\n",
      "Total shape of the sample dataset: (353131, 116)\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "1    325334\n",
      "0     27797\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data preprocessing complete.\n",
      "Shape of the final scaled features (X_scaled): (353131, 115)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# --- 1. Define the path and the sampling fraction ---\n",
    "path = '../data/N-BaLot/' \n",
    "sample_fraction = 0.05 # We'll take a 5% random sample from each file\n",
    "\n",
    "# --- 2. Loop through files, load a sample from each, and combine ---\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "print(f\"Found {len(all_files)} files. Sampling {sample_fraction*100}% from each...\")\n",
    "\n",
    "df_list = []\n",
    "for i, filename in enumerate(all_files):\n",
    "    if 'data_summary' in filename or 'device_info' in filename or 'features' in filename:\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing file {i+1}/{len(all_files)}: {os.path.basename(filename)}\")\n",
    "    \n",
    "    # Load the full file\n",
    "    df_temp = pd.read_csv(filename)\n",
    "    \n",
    "    # Take a random sample from this file\n",
    "    df_sample_temp = df_temp.sample(frac=sample_fraction, random_state=42)\n",
    "    \n",
    "    # Create the label\n",
    "    if 'benign' in filename:\n",
    "        df_sample_temp['label'] = 0\n",
    "    else:\n",
    "        df_sample_temp['label'] = 1\n",
    "        \n",
    "    df_list.append(df_sample_temp)\n",
    "\n",
    "print(\"\\nCombining all samples...\")\n",
    "df_sample = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "print(\"\\nCombined sample created successfully!\")\n",
    "print(f\"Total shape of the sample dataset: {df_sample.shape}\")\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df_sample['label'].value_counts())\n",
    "\n",
    "# --- 3. Preprocess the final sample ---\n",
    "X_sample = df_sample.drop('label', axis=1)\n",
    "y_sample = df_sample['label']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_sample)\n",
    "\n",
    "print(\"\\nData preprocessing complete.\")\n",
    "print(f\"Shape of the final scaled features (X_scaled): {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cedda67-3920-439e-beee-3ebc5ca48046",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_combined' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# --- 1. Separate features (X) and labels (y) ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m X = \u001b[43mdf_combined\u001b[49m.drop(\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m      6\u001b[39m y = df_combined[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# --- 2. Create a smaller, representative sample (e.g., 300,000 records) ---\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# We use train_test_split as a clever way to get a stratified sample.\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 'stratify=y' ensures the sample has the same percentage of anomalies as the original dataset.\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_combined' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Separate features (X) and labels (y) ---\n",
    "X = df_combined.drop('label', axis=1)\n",
    "y = df_combined['label']\n",
    "\n",
    "# --- 2. Create a smaller, representative sample (e.g., 300,000 records) ---\n",
    "# We use train_test_split as a clever way to get a stratified sample.\n",
    "# 'stratify=y' ensures the sample has the same percentage of anomalies as the original dataset.\n",
    "_, X_sample, _, y_sample = train_test_split(X, y, test_size=300000, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Created a representative sample of the data.\")\n",
    "print(f\"Shape of the sample features (X_sample): {X_sample.shape}\")\n",
    "print(f\"Sample label distribution:\\n{y_sample.value_counts()}\")\n",
    "\n",
    "\n",
    "# --- 3. Scale the numerical features of the sample ---\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_sample)\n",
    "\n",
    "print(\"\\nData preprocessing complete.\")\n",
    "print(f\"Shape of the final scaled features (X_scaled): {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c23f5b-3f2f-49a6-af91-4735c76facc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
