{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e876285-ac6c-47c9-a972-654a5649178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files loaded and combined successfully!\n",
      "Total shape of the new dataset: (7062606, 116)\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "1    6506674\n",
      "0     555932\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 5 rows of the combined dataset:\n",
      "   MI_dir_L5_weight  MI_dir_L5_mean  MI_dir_L5_variance  MI_dir_L3_weight  \\\n",
      "0          1.000000       98.000000        0.000000e+00          1.000000   \n",
      "1          1.029000       98.000000        1.818989e-12          1.119520   \n",
      "2          1.504156       76.725612        2.281808e+02          1.729662   \n",
      "3          2.460087       75.617679        1.372200e+02          2.699075   \n",
      "4          3.460055       75.150149        9.809937e+01          3.699054   \n",
      "\n",
      "   MI_dir_L3_mean  MI_dir_L3_variance  MI_dir_L1_weight  MI_dir_L1_mean  \\\n",
      "0       98.000000            0.000000          1.000000       98.000000   \n",
      "1       98.000000            0.000000          1.492583       98.000000   \n",
      "2       79.499272          249.746357          2.294102       84.051188   \n",
      "3       77.461807          164.269331          3.280499       80.987267   \n",
      "4       76.525944          122.224798          4.280490       79.354915   \n",
      "\n",
      "   MI_dir_L1_variance  MI_dir_L0.1_weight  ...  HpHp_L0.1_covariance  \\\n",
      "0        0.000000e+00            1.000000  ...                   0.0   \n",
      "1        3.637979e-12            1.931640  ...                   0.0   \n",
      "2        2.517926e+02            2.904273  ...                   0.0   \n",
      "3        1.964467e+02            3.902546  ...                   0.0   \n",
      "4        1.592943e+02            4.902545  ...                   0.0   \n",
      "\n",
      "   HpHp_L0.1_pcc  HpHp_L0.01_weight  HpHp_L0.01_mean  HpHp_L0.01_std  \\\n",
      "0            0.0           1.000000             98.0        0.000000   \n",
      "1            0.0           1.992944             98.0        0.000001   \n",
      "2            0.0           1.000000             66.0        0.000000   \n",
      "3            0.0           1.000000             74.0        0.000000   \n",
      "4            0.0           1.000000             74.0        0.000000   \n",
      "\n",
      "   HpHp_L0.01_magnitude  HpHp_L0.01_radius  HpHp_L0.01_covariance  \\\n",
      "0             98.000000       0.000000e+00                    0.0   \n",
      "1            138.592929       1.818989e-12                    0.0   \n",
      "2            114.856432       0.000000e+00                    0.0   \n",
      "3             74.000000       0.000000e+00                    0.0   \n",
      "4             74.000000       0.000000e+00                    0.0   \n",
      "\n",
      "   HpHp_L0.01_pcc  label  \n",
      "0             0.0      1  \n",
      "1             0.0      1  \n",
      "2             0.0      1  \n",
      "3             0.0      1  \n",
      "4             0.0      1  \n",
      "\n",
      "[5 rows x 116 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# --- 1. Define the CORRECT path to your data folder ---\n",
    "# The '../' tells Python to go up one directory from the 'notebooks' folder\n",
    "path = '../data/N-BaLot/' \n",
    "\n",
    "# --- 2. Get a list of all CSV files in the folder ---\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "# --- 3. Loop through the files, load them, and add a label ---\n",
    "df_list = []\n",
    "for filename in all_files:\n",
    "    # Skip non-data files\n",
    "    if 'data_summary' in filename or 'device_info' in filename or 'features' in filename:\n",
    "        continue\n",
    "        \n",
    "    # Read the current CSV file\n",
    "    df_temp = pd.read_csv(filename)\n",
    "    \n",
    "    # Create the label: 0 for benign, 1 for attack\n",
    "    if 'benign' in filename:\n",
    "        df_temp['label'] = 0\n",
    "    else:\n",
    "        df_temp['label'] = 1\n",
    "        \n",
    "    df_list.append(df_temp)\n",
    "\n",
    "# --- 4. Combine all the individual dataframes into one ---\n",
    "df_combined = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# --- 5. Verify the result ---\n",
    "print(\"All files loaded and combined successfully!\")\n",
    "print(f\"Total shape of the new dataset: {df_combined.shape}\")\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df_combined['label'].value_counts())\n",
    "print(\"\\nFirst 5 rows of the combined dataset:\")\n",
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cedda67-3920-439e-beee-3ebc5ca48046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Separate features (X) and labels (y) ---\n",
    "X = df_combined.drop('label', axis=1)\n",
    "y = df_combined['label']\n",
    "\n",
    "# --- 2. Create a smaller, representative sample (e.g., 300,000 records) ---\n",
    "# We use train_test_split as a clever way to get a stratified sample.\n",
    "# 'stratify=y' ensures the sample has the same percentage of anomalies as the original dataset.\n",
    "_, X_sample, _, y_sample = train_test_split(X, y, test_size=300000, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Created a representative sample of the data.\")\n",
    "print(f\"Shape of the sample features (X_sample): {X_sample.shape}\")\n",
    "print(f\"Sample label distribution:\\n{y_sample.value_counts()}\")\n",
    "\n",
    "\n",
    "# --- 3. Scale the numerical features of the sample ---\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_sample)\n",
    "\n",
    "print(\"\\nData preprocessing complete.\")\n",
    "print(f\"Shape of the final scaled features (X_scaled): {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c23f5b-3f2f-49a6-af91-4735c76facc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
